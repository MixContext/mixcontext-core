{"version":3,"sources":["../../cli/index.ts","../../src/parsers/pdf.ts","../../src/utils/hash.ts","../../src/chunk.ts","../../src/plugin.ts","../../src/parsers/registry.ts","../../src/parsers/docx.ts","../../src/parsers/html.ts","../../src/parsers/image.ts","../../src/parsers/index.ts","../../src/errors.ts","../../src/bundle.ts","../../src/tokens.ts","../../src/embed/providers/openai.ts","../../src/embed/providers/local.ts","../../src/embed/index.ts","../../src/vector/local.ts"],"sourcesContent":["#!/usr/bin/env node\nimport { Command } from 'commander';\nimport fg from 'fast-glob';\nimport { promises as fs } from 'fs';\nimport path from 'path';\nimport chalkPkg, { Chalk } from 'chalk';\n// @ts-ignore – typings provided via @types/cli-progress (added in package.json)\nimport cliProgress from 'cli-progress';\nimport { parseFile } from '../src/parsers';\nimport { buildBundle } from '../src/bundle';\nimport { estimateTokens } from '../src/tokens';\nimport { LocalVectorDB } from '../src/vector/local';\n\nlet chalk = chalkPkg;\n\nasync function main() {\n  const program = new Command();\n\n  program\n    .name('mixcontext')\n    .description('Generate MixContext bundles from documents')\n    .argument('[inputs...]', 'Input files – accepts glob patterns')\n    .option('--zip', 'Include originals folder in bundle')\n    .option('--embed <provider>', 'Generate embeddings using provider (openai|local)')\n    .option('--openai-key <key>', 'OpenAI API key (required for --embed openai)')\n    .option('--ocr', 'Enable OCR during parsing (images, scanned PDFs)')\n    .option('--model <name>', 'Embedding model name override')\n    .option('--local-endpoint <url>', 'Base URL to local embedding HTTP service')\n    .option('--out <file>', 'Output bundle path (only used when a single input file is given)')\n    .option('--init', 'Generate typed mixcontext.config.ts template')\n    .option('--no-color', 'Disable colored output')\n    .option('--vector-db <driver>', 'Vector database driver (local)')\n    .option('--db <file>', 'Vector DB file path (for local driver)')\n    .addHelpText('after', `\nExamples:\n  $ mixcontext docs/*.pdf\n  $ mixcontext \"**/*.md\" --zip --embed openai --openai-key $OPENAI_KEY\n`)\n    .parse();\n\n  const opts = program.opts();\n  const patterns = program.args as string[];\n\n  // Vector DB instance that will be lazily created once we know the vector dimensionality.\n  let vdb: LocalVectorDB | undefined;\n\n  if (!opts.init && patterns.length === 0) {\n    console.error(chalk.red('No input files specified.'));\n    process.exit(2);\n  }\n\n  // Disable colors when the flag is provided so tests can easily match output.\n  if (opts.noColor) {\n    chalk = new Chalk({ level: 0 });\n  }\n\n  // `mixcontext --init` : write default configuration template then exit.\n  if (opts.init) {\n    const configPath = path.resolve(process.cwd(), 'mixcontext.config.ts');\n    let exists = false;\n    try {\n      await fs.access(configPath);\n      exists = true;\n    } catch {\n      /* not exist */\n    }\n\n    if (!exists) {\n      const template = `// Auto-generated by mixcontext --init\\n` +\n        `// Learn more: https://github.com/mixcontext\\n\\n` +\n        `import type { BuildOpts } from 'mixcontext-core';\\n\\n` +\n        `const config: BuildOpts = {\\n` +\n        `  /** Include a folder with raw input files inside the bundle */\\n` +\n        `  zipOriginals: true,\\n\\n` +\n        `  /** Configure vector embeddings */\\n` +\n        `  embed: {\\n` +\n        `    provider: 'openai',\\n` +\n        `    apiKey: process.env.OPENAI_KEY!,\\n` +\n        `    model: 'text-embedding-3-small'\\n` +\n        `  },\\n\\n` +\n        `  /** Enable OCR when parsing images or scanned PDFs */\\n` +\n        `  ocr: false,\\n\\n` +\n        `  /** Enable aggressive duplicate-chunk removal */\\n` +\n        `  dedupe: true\\n` +\n        `};\\n\\nexport default config;\\n`;\n\n      await fs.writeFile(configPath, template, 'utf8');\n      console.log(chalk.green(`Created ${path.relative(process.cwd(), configPath)}`));\n    } else {\n      console.log(chalk.yellow('mixcontext.config.ts already exists – nothing to do.'));\n    }\n\n    process.exit(0);\n  }\n\n  // Expand globs to absolute file paths.\n  const matched = await fg(patterns, { absolute: true, dot: false });\n  const uniqueFiles = Array.from(new Set(matched));\n\n  if (uniqueFiles.length === 0) {\n    console.error(chalk.red('No input files matched the given patterns.'));\n    process.exit(2);\n  }\n\n  // Progress bar\n  const multibar = new cliProgress.MultiBar(\n    {\n      clearOnComplete: false,\n      hideCursor: true,\n      format: `{bar} {percentage}% | {value}/{total} | {label}`\n    },\n    cliProgress.Presets.shades_classic\n  );\n\n  const parseBar = multibar.create(uniqueFiles.length, 0, { label: 'parse' });\n  const embedBar = opts.embed ? multibar.create(uniqueFiles.length, 0, { label: 'embed' }) : undefined;\n\n  // Track summary\n  type Summary = {\n    file: string;\n    tokens: number;\n    chunks: number;\n    duplicates: number;\n    bytes: number;\n    skipped: boolean;\n    error?: unknown;\n  };\n  const summary: Summary[] = [];\n\n  let fatal = false;\n\n  for (const file of uniqueFiles) {\n    try {\n      const buf = await fs.readFile(file);\n      const buildOpts: import('../src/config').BuildOpts = {\n        zipOriginals: opts.zip,\n        embed: opts.embed\n          ? (\n              opts.embed === 'openai'\n                ? {\n                    provider: 'openai',\n                    apiKey: opts.openaiKey,\n                    model: opts.model\n                  }\n                : {\n                    provider: 'local',\n                    endpoint: opts.localEndpoint,\n                    model: opts.model\n                  }\n            )\n          : undefined,\n        ocr: opts.ocr as boolean | undefined\n      };\n\n      const doc = await parseFile(buf, buildOpts);\n      doc.filename = path.basename(file);\n\n      // Ensure chunks / duplicates.\n      if (!doc.chunks) {\n        console.error(chalk.yellow(`Parser for ${file} did not return chunks; skipping.`));\n        summary.push({ file, tokens: 0, chunks: 0, duplicates: 0, bytes: 0, skipped: true });\n        continue;\n      }\n\n      const tokens = estimateTokens(doc.text);\n      const chunks = doc.chunks.length;\n      const duplicates = doc.chunks.filter(c => (c as any).dupOf !== undefined).length;\n\n      // Build bundle (includes optional embedding)\n      const zipBuf = await buildBundle([doc], buildOpts);\n\n      // ---- Vector DB upsert ----\n      if (opts.vectorDb === 'local' && opts.embed && doc.chunks) {\n        for (const chunk of doc.chunks) {\n          if (!chunk.embedding) continue;\n\n          // Lazily create LocalVectorDB using the first embedding's dimension.\n          if (!vdb) {\n            vdb = new LocalVectorDB({ dim: chunk.embedding.length, dbPath: opts.db });\n          }\n\n          const chunkId = `${doc.id ?? path.basename(file)}-${chunk.idx}`;\n          vdb.upsert(chunkId, chunk.embedding, { file: doc.filename, chunkIdx: chunk.idx });\n        }\n      }\n\n      // Update embedding progress bar once per file when embeddings are enabled.\n      if (embedBar) {\n        embedBar.increment(1, { label: 'embed' });\n      }\n\n      // Destination path – same dir, basename + .mcx.zip\n      let dest: string;\n      if (opts.out && uniqueFiles.length === 1) {\n        dest = path.resolve(String(opts.out));\n      } else {\n        dest = path.join(\n          path.dirname(file),\n          `${path.basename(file, path.extname(file))}.mcx.zip`\n        );\n      }\n      await fs.writeFile(dest, zipBuf);\n      const bytes = zipBuf.length;\n\n      summary.push({ file, tokens, chunks, duplicates, bytes, skipped: false });\n    } catch (err) {\n      summary.push({ file, tokens: 0, chunks: 0, duplicates: 0, bytes: 0, skipped: true, error: err });\n      console.error(chalk.red(`Error processing ${file}: ${err instanceof Error ? err.message : err}`));\n      // Continue processing remaining files but mark as skipped.\n    } finally {\n      // Parsing for this file is complete\n      parseBar.increment(1, { label: 'parse' });\n    }\n  }\n\n  multibar.stop();\n\n  // Print summary table\n  const totalProcessed = summary.filter(s => !s.skipped).length;\n  const totalSkipped = summary.filter(s => s.skipped).length;\n\n  console.log(chalk.bold('\\nSummary')); // newline before\n  console.table(\n    summary.map(s => ({\n      File: path.basename(s.file),\n      Tokens: s.tokens,\n      Chunks: s.chunks,\n      Duplicates: s.duplicates,\n      Bytes: s.bytes,\n      Skipped: s.skipped ? 'yes' : ''\n    }))\n  );\n\n  if (totalSkipped > 0) {\n    console.error(chalk.yellow(`${totalSkipped} file(s) skipped due to handled errors.`));\n    process.exit(1);\n  } else if (fatal) {\n    process.exit(2);\n  } else {\n    process.exit(0);\n  }\n}\n\nmain().catch(err => {\n  console.error(chalk.red(err instanceof Error ? err.stack || err.message : String(err)));\n  process.exit(2);\n});\n","// Copyright 2025 MixContext.ai\n// SPDX-License-Identifier: MIT\n\nimport { MCXDocument } from '../types';\nimport { v4 as uuid } from 'uuid';\nimport { chunkText } from '../chunk';\n// Using legacy build for Node to avoid DOM dependencies.\nimport { getDocument } from 'pdfjs-dist/legacy/build/pdf.mjs';\nimport { registerParser } from './registry';\nimport fs from 'node:fs';\nimport crypto from 'node:crypto';\nimport { ParserInput } from '../types';\n\nexport async function parsePdf(input: ParserInput, _opts: { ocr?: boolean } = {}): Promise<MCXDocument> {\n  /* _opts.ocr is currently ignored – OCR for scanned PDFs will be added in future. */\n\n  let pdf: any;\n  let originalSize = 0;\n\n  if (typeof input === 'string') {\n    const stat = await fs.promises.stat(input);\n    originalSize = stat.size;\n    pdf = await (getDocument as any)({\n      url: input,\n      disableWorker: true,\n      useSystemFonts: true\n    }).promise;\n  } else {\n    const buf = input as Buffer;\n    originalSize = buf.length;\n    const uint8Data = new Uint8Array(buf.buffer, buf.byteOffset, buf.byteLength);\n    pdf = await (getDocument as any)({\n      data: uint8Data,\n      disableWorker: true,\n      useSystemFonts: true\n    }).promise;\n  }\n\n  const pages: string[] = [];\n  for (let pageNum = 1; pageNum <= pdf.numPages; pageNum++) {\n    const page = await pdf.getPage(pageNum);\n    const textContent = await page.getTextContent();\n    // Each item in `items` has a `str` property that contains text fragment.\n    const pageText = textContent.items\n      .map((i: any) => i.str as string)\n      .join(' ');\n    pages.push(pageText.trim());\n  }\n\n  const text = pages.join('\\n\\n');\n\n  const doc: MCXDocument & { original?: Buffer; originalPath?: string } = {\n    id: uuid(),\n    filename: 'unknown.pdf',\n    mimetype: 'application/pdf',\n    text,\n    chunks: await chunkText(text),\n    meta: { pages: pdf.numPages, originalSize }\n  };\n\n  if (typeof input === 'string') {\n    doc.originalPath = input;\n\n    // compute sha256 hash while streaming to avoid buffering\n    try {\n      const hash = crypto.createHash('sha256');\n      await new Promise<void>((resolve, reject) => {\n        const rs = fs.createReadStream(input);\n        rs.on('data', chunk => hash.update(chunk));\n        rs.on('error', reject);\n        rs.on('end', () => {\n          doc.meta = { ...(doc.meta || {}), sha256: hash.digest('hex') };\n          resolve();\n        });\n      });\n    } catch {/* non-fatal */}\n  } else {\n    doc.original = input as Buffer; // keep raw binary when bundling originals\n  }\n  return doc;\n}\n\n// Register parser on import.\nregisterParser('application/pdf', parsePdf);\n","export async function hashText(text: string): Promise<string> {\n  // Prefer Web Crypto API when available (browsers, recent Node versions)\n  if (typeof globalThis.crypto?.subtle?.digest === 'function') {\n    const encoder = new TextEncoder();\n    const data = encoder.encode(text);\n    const hashBuf = await globalThis.crypto.subtle.digest('SHA-256', data);\n    // Convert ArrayBuffer to hex string\n    const hashArray = Array.from(new Uint8Array(hashBuf));\n    return hashArray.map(b => b.toString(16).padStart(2, '0')).join('');\n  }\n\n  // Fallback to Node's crypto module (synchronous but wrapped in Promise)\n  // Using dynamic import so bundlers can tree-shake for browser builds.\n  const { createHash } = await import('node:crypto');\n  return createHash('sha256').update(text).digest('hex');\n}\n","// Copyright 2025 MixContext.ai\n// SPDX-License-Identifier: MIT\n\nimport { MCXChunk } from './types';\nimport { hashText } from './utils/hash';\n\n/**\n * Split text into ~`charLimit`-sized chunks, respecting sentence boundaries.\n * Falls back to a simple regex when `Intl.Segmenter` isn't available\n * (e.g. older Node versions).\n */\nexport async function chunkText(\n  text: string,\n  charLimit = 3000,\n  opts: { dedupe?: boolean } = {}\n): Promise<MCXChunk[]> {\n  const segments: string[] = [];\n\n  if (typeof (Intl as any)?.Segmenter === 'function') {\n    const SegClass = (Intl as any).Segmenter;\n    const seg = new SegClass('en', { granularity: 'sentence' });\n    for (const { segment } of seg.segment(text)) {\n      segments.push(segment);\n    }\n  } else {\n    // Simple fallback – split on punctuation followed by whitespace.\n    segments.push(...text.split(/(?<=[.?!])\\s+/));\n  }\n\n  const chunks: MCXChunk[] = [];\n  let buffer = '';\n  let _idx = 0;\n\n  const push = () => {\n    if (buffer.trim().length) {\n      chunks.push({ idx: chunks.length, text: buffer.trim() });\n      buffer = '';\n      _idx++;\n    }\n  };\n\n  for (const sentence of segments) {\n    if (buffer.length + sentence.length <= charLimit) {\n      buffer += (buffer ? ' ' : '') + sentence.trim();\n    } else {\n      push();\n      // If single sentence longer than limit, hard-slice it.\n      if (sentence.length > charLimit) {\n        for (let i = 0; i < sentence.length; i += charLimit) {\n          buffer = sentence.slice(i, i + charLimit);\n          push();\n        }\n      } else {\n        buffer = sentence.trim();\n      }\n    }\n  }\n  push();\n\n  // Deduplicate identical chunks by SHA-256 hash if enabled.\n  if (opts.dedupe !== false) {\n    const seen = new Map<string, number>();\n    for (const c of chunks) {\n      if (!c.text) continue; // Skip empty (shouldn't happen pre-dedupe)\n      const hash = await hashText(c.text);\n      const firstIdx = seen.get(hash);\n      if (firstIdx !== undefined) {\n        // Mark as duplicate of the first occurrence – clear text to save bytes.\n        (c as any).dupOf = firstIdx;\n        c.text = '';\n      } else {\n        seen.set(hash, c.idx);\n      }\n    }\n  }\n\n  return chunks;\n}\n","import type { MCXDocument, ParserInput } from './types.js';\n\nexport interface ParserPlugin {\n  /** MIME type string or regular expression this plugin can handle. */\n  mime: string | RegExp;\n  /**\n   * Parse the supplied file buffer into an MCXDocument.\n   * Implementations should throw if parsing fails.\n   */\n  parse(input: ParserInput): Promise<MCXDocument>;\n}\n\n// Internal registry of runtime-registered plugins.\nconst plugins: ParserPlugin[] = [];\n\n/**\n * Register a new parser plugin during application startup.\n */\nexport function registerPlugin(p: ParserPlugin): void {\n  plugins.push(p);\n}\n\n/**\n * Retrieve the first plugin that matches the given MIME type.\n */\nexport function getPlugin(mime: string): ParserPlugin | undefined {\n  return plugins.find((p) =>\n    typeof p.mime === 'string' ? p.mime === mime : p.mime.test(mime),\n  );\n}\n","import type { ParserFn } from '../types';\nimport { getPlugin } from '../plugin';\n\n// Internal store maps lowercase mime-type -> parser function\nconst registry = new Map<string, ParserFn>();\n\nexport function registerParser(mime: string, fn: ParserFn): void {\n  registry.set(mime.toLowerCase(), fn);\n}\n\nexport function getParser(mime: string): ParserFn | undefined {\n  // 1. Check built-in parser registry first.\n  const internal = registry.get(mime.toLowerCase());\n  if (internal) return internal;\n\n  // 2. Fall back to runtime-registered plugins.\n  const plugin = getPlugin(mime);\n  return plugin?.parse;\n}\n","// Copyright 2025 MixContext.ai\n// SPDX-License-Identifier: MIT\n\nimport { MCXDocument, ParserInput } from '../types';\nimport { v4 as uuid } from 'uuid';\nimport mammoth from 'mammoth';\nimport { chunkText } from '../chunk';\nimport fs from 'node:fs';\nimport unzipper from 'unzipper';\nimport sax from 'sax';\nimport { registerParser } from './registry';\n\nexport async function parseDocx(input: ParserInput): Promise<MCXDocument> {\n  let text = '';\n  let originalSize = 0;\n\n  if (typeof input === 'string') {\n    const stat = await fs.promises.stat(input);\n    originalSize = stat.size;\n\n    // Stream word/document.xml only\n    const rs = fs.createReadStream(input);\n    const zipStream = rs.pipe(unzipper.ParseOne(/word\\/document.xml/));\n\n    text = await extractTextFromDocumentXml(zipStream);\n  } else {\n    const buf = input as Buffer;\n    originalSize = buf.length;\n    const result = await mammoth.extractRawText({ buffer: buf });\n    text = result.value.trim();\n  }\n\n  const doc: MCXDocument & { original?: Buffer; originalPath?: string } = {\n    id: uuid(),\n    filename: 'unknown.docx',\n    mimetype: 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',\n    text,\n    chunks: await chunkText(text),\n    meta: { originalSize }\n  };\n\n  if (typeof input === 'string') {\n    doc.originalPath = input;\n  } else {\n    doc.original = input as Buffer;\n  }\n\n  return doc;\n}\n\nasync function extractTextFromDocumentXml(stream: NodeJS.ReadableStream): Promise<string> {\n  return new Promise<string>((resolve, reject) => {\n    const parser = sax.createStream(true, {});\n    let textParts: string[] = [];\n    parser.on('text', (t: string) => {\n      textParts.push(t);\n    });\n    parser.on('error', reject);\n    parser.on('end', () => {\n      resolve(textParts.join(' ').replace(/\\s+/g, ' ').trim());\n    });\n    stream.pipe(parser);\n  });\n}\n\n// Register parser on import.\nregisterParser('application/vnd.openxmlformats-officedocument.wordprocessingml.document', parseDocx);\n","// Copyright 2025 MixContext.ai\n// SPDX-License-Identifier: MIT\n\nimport { MCXDocument, ParserInput } from '../types';\nimport { v4 as uuid } from 'uuid';\nimport { extractFromHtml } from '@extractus/article-extractor';\nimport { chunkText } from '../chunk';\nimport { registerParser } from './registry';\nimport chardet from 'chardet';\nimport iconv from 'iconv-lite';\nimport { franc } from 'franc-min';\nimport { promises as fs } from 'node:fs';\n\n/**\n * Parse raw HTML (or UTF-8 buffer) and return a MixContext document.\n */\nexport async function parseHtml(input: ParserInput): Promise<MCXDocument> {\n  // Normalize input to a Buffer. When a string path is supplied, read from disk.\n  const buf: Buffer =\n    typeof input === 'string' ? await fs.readFile(input) : (input as Buffer);\n\n  // Detect character set and decode accordingly. Default to UTF-8.\n  let encoding = 'utf8';\n  try {\n    const detected = chardet.detect(buf);\n    if (typeof detected === 'string' && detected) {\n      encoding = detected.toLowerCase();\n    }\n  } catch {\n    // Fallback – keep UTF-8.\n  }\n\n  let html: string;\n  try {\n    html = iconv.decode(buf, encoding);\n  } catch {\n    // If decoding fails, fall back to UTF-8.\n    html = buf.toString('utf8');\n  }\n\n  // `article-extractor` can throw when markup is invalid; fall back to plain\n  // text stripping if needed.\n  let title = 'Untitled';\n  let content = '';\n  let site: string | undefined;\n\n  try {\n    const result = await extractFromHtml(html);\n\n    if (result) {\n      title = result.title ?? title;\n      content = (result.content ?? '').trim();\n      if (result.source) site = result.source;\n    }\n  } catch {\n    // graceful degradation – strip HTML tags.\n  }\n\n  if (!content) {\n    content = html.replace(/<[^>]+>/g, ' ');\n  }\n\n  const text = content.replace(/\\s+/g, ' ').trim();\n\n  // Detect primary language of the extracted text.\n  let lang: string | undefined;\n  try {\n    const guess = franc(text.slice(0, 10_000), { minLength: 10 });\n    if (guess && guess !== 'und') lang = guess;\n  } catch {\n    /* noop */\n  }\n\n  const meta: Record<string, any> = { title };\n  if (site) meta.site = site;\n  if (lang) meta.lang = lang;\n\n  const doc: MCXDocument = {\n    id: uuid(),\n    filename: 'unknown.html',\n    mimetype: 'text/html',\n    text,\n    chunks: await chunkText(text),\n    meta\n  };\n\n  return doc;\n}\n\nregisterParser('text/html', parseHtml);\nregisterParser('application/xhtml+xml', parseHtml);\n","// Copyright 2025 MixContext.ai\n// SPDX-License-Identifier: MIT\n\nimport { MCXDocument } from '../types';\nimport { v4 as uuid } from 'uuid';\nimport { registerParser } from './registry';\nimport { chunkText } from '../chunk';\n\n// Lazily-initialised shared Tesseract worker so we pay the WASM load cost only once.\nimport { createWorker, type Worker as TesseractWorker } from 'tesseract.js';\nimport { EventEmitter } from 'events';\n\nlet worker: TesseractWorker | null = null;\n\n/**\n * Spawn the global Tesseract worker on-demand (singleton).\n */\nasync function initWorker(): Promise<TesseractWorker> {\n  if (worker) return worker;\n  worker = await createWorker('eng');\n  return worker;\n}\n\nexport interface ImageParseOptions {\n  /** If true run OCR, otherwise skip to keep things fast/cheap. */\n  ocr?: boolean;\n}\n\nexport async function parseImage(\n  buf: Buffer,\n  opts: ImageParseOptions = {},\n  events?: EventEmitter\n): Promise<MCXDocument> {\n  // When OCR is explicitly disabled just return an empty doc – this allows\n  // callers to quickly inspect image metadata without paying the heavyweight\n  // Tesseract startup cost.\n  if (!opts.ocr) {\n    return {\n      id: uuid(),\n      filename: 'unknown.jpg',\n      mimetype: 'image/jpeg',\n      text: '',\n      meta: { ocr: false }\n    };\n  }\n\n  const w = await initWorker();\n\n  // Bridge Tesseract's progress callbacks → EventEmitter for CLI/UI consumption.\n  const { data } = await (w as any).recognize(buf, {\n    logger: (m: any) => {\n      if (events) events?.emit('progress', m);\n    }\n  } as any);\n\n  const text = data.text.trim();\n  const lines = text.split(/\\r?\\n/).filter(Boolean).length;\n\n  const doc: MCXDocument = {\n    id: uuid(),\n    filename: 'unknown.jpg',\n    mimetype: 'image/jpeg',\n    text,\n    chunks: await chunkText(text),\n    meta: {\n      ocr: true,\n      lines,\n      confidence: data.confidence\n    }\n  };\n\n  return doc;\n}\n\n// Self-register this parser for common bitmap formats.\nregisterParser('image/png', parseImage as any);\nregisterParser('image/jpeg', parseImage as any);\n","// re-export individual parsers\nexport { parsePdf } from './pdf';\nexport { parseDocx } from './docx';\nexport { parseHtml } from './html';\nexport { parseImage } from './image';\n\nimport { MCXDocument } from '../types';\nimport { fileTypeFromBuffer } from 'file-type';\nimport { getParser } from './registry';\nimport { UnsupportedMimeError } from '../errors';\nimport type { BuildOpts } from '../config';\nimport { promises as fs } from 'node:fs';\nimport path from 'node:path';\nimport * as mime from 'mime-types';\n\n// If an input file is larger than this threshold (bytes), we avoid buffering it\n// in memory and instead operate on a temporary file path.\nconst STREAM_THRESHOLD = 25 * 1024 * 1024; // 25 MB\n\nexport async function parseFile(\n  input: File | Buffer | string,\n  _opts: BuildOpts = {}\n): Promise<MCXDocument> {\n  // Scenario A: caller already provided a string path (used by CLI for huge files)\n  if (typeof input === 'string') {\n    const absPath = path.resolve(input);\n    const mimeGuess = mime.lookup(absPath) || 'application/octet-stream';\n    const parser = getParser(mimeGuess as string);\n    if (!parser) throw new UnsupportedMimeError(`Unsupported mime: ${mimeGuess}`);\n    const doc = await parser(absPath, _opts);\n    (doc as any).originalPath = absPath;\n    return doc;\n  }\n\n  // Scenario B: File (browser) or Buffer (Node) – decide whether to stream.\n\n  let buf: Buffer;\n  if (input instanceof Buffer) {\n    buf = input;\n  } else {\n    // DOM File – no \"size\" property in TS yet, but we rely on it at runtime.\n    const fileLike: any = input;\n    if (fileLike.size >= STREAM_THRESHOLD) {\n      // Create tmp file and stream write into it.\n      const { path: tmpPath, cleanup } = await writeTmpFile(await fileLike.arrayBuffer());\n      const mimeFromName = fileLike.type || mime.lookup(fileLike.name) || 'application/octet-stream';\n      const parser = getParser(mimeFromName as string);\n      if (!parser) throw new UnsupportedMimeError(`Unsupported mime: ${mimeFromName}`);\n      const doc = await parser(tmpPath, _opts);\n      (doc as any).originalPath = tmpPath;\n      // Allow caller to clean up later; we attach cleanup function\n      (doc as any)._cleanupTmp = cleanup;\n      return doc;\n    }\n    buf = Buffer.from(await fileLike.arrayBuffer());\n  }\n\n  // Buffer path (size may still be > threshold if provided directly)\n  if (buf.length >= STREAM_THRESHOLD) {\n    const { path: tmpPath } = await writeTmpFile(buf);\n    return await parseFile(tmpPath, _opts); // recurse into string path branch\n  }\n\n  let type: Awaited<ReturnType<typeof fileTypeFromBuffer>>;\n  try {\n    type = await fileTypeFromBuffer(buf);\n  } catch {\n    type = undefined;\n  }\n\n  let mimeType = type?.mime || (input instanceof File ? (input as any).type : 'application/octet-stream');\n  if ((mimeType === 'application/octet-stream' || mimeType === 'text/plain') && buf.length > 0) {\n    const sample = buf.slice(0, 4096).toString('utf8');\n    if (sample.includes(',') && sample.includes('\\n') && !sample.includes('\\0')) {\n      mimeType = 'text/csv';\n    }\n  }\n\n  const parser = getParser(mimeType);\n  if (!parser) throw new UnsupportedMimeError(`Unsupported mime: ${mimeType}`);\n\n  const doc: MCXDocument = await parser(buf, _opts);\n  (doc as any).original = buf;\n  doc.meta = { ...(doc.meta || {}), originalSize: buf.length };\n  return doc;\n}\n\n// Helper: write a buffer to a tmp file and return its path + cleanup function.\nasync function writeTmpFile(data: Buffer | ArrayBuffer): Promise<{ path: string; cleanup: () => Promise<void> }> {\n  const { mkdtemp, writeFile, rm } = fs;\n  const os = await import('node:os');\n  const dir = await mkdtemp(path.join(os.tmpdir(), 'mixcx-'));\n  const tmpPath = path.join(dir, 'input');\n  await writeFile(tmpPath, data instanceof Buffer ? data : Buffer.from(data));\n  return { path: tmpPath, cleanup: () => rm(dir, { recursive: true, force: true }) };\n}\n","import en from './i18n/en';\n\n// Custom error types surfaced by MixContext parsers\n\n// New interface describing additional context information for an error\nexport interface ErrorContext {\n  filename?: string;\n  mime?: string;\n}\n\n// Helper that decorates an error with additional context and localized message\nexport function wrapErr(err: Error, ctx: ErrorContext): Error {\n  const { filename, mime } = ctx;\n\n  // Determine base (possibly localized) message depending on the error type\n  let localizedMessage = err.message;\n  if (err instanceof UnsupportedMimeError && mime) {\n    localizedMessage = en.unsupportedMime.replace('{mime}', mime);\n  } else if (err instanceof FileTooLargeError && filename) {\n    localizedMessage = en.tooLarge.replace('{filename}', filename);\n  }\n\n  // Prepend context information – filename then mime (if provided)\n  const prefixParts: string[] = [];\n  if (filename) prefixParts.push(filename);\n  if (mime) prefixParts.push(mime);\n  const prefix = prefixParts.join(' ');\n  const message = prefix ? `${prefix}: ${localizedMessage}` : localizedMessage;\n\n  // Re-instantiate the same error type with the enriched message\n  const WrappedError = (err as any).constructor as new (msg: string) => Error;\n  const wrapped = new WrappedError(message);\n  // Preserve original stack trace where possible\n  wrapped.stack = err.stack;\n  // Copy additional enumerable properties (e.g., mime)\n  Object.assign(wrapped, err, ctx);\n  return wrapped;\n}\n\nexport class UnsupportedMimeError extends Error {\n  mime?: string;\n  constructor(mime: string, message = 'Unsupported MIME type') {\n    super(message);\n    this.name = 'UnsupportedMimeError';\n    this.mime = mime;\n  }\n}\n\nexport class FileTooLargeError extends Error {\n  constructor(message = 'File exceeds maximum size') {\n    super(message);\n    this.name = 'FileTooLargeError';\n  }\n}\n","// Copyright 2025 MixContext.ai\n// SPDX-License-Identifier: MIT\n\nimport { MCXDocument } from './types';\nimport archiver from 'archiver';\nimport { PassThrough } from 'stream';\nimport { chunkText } from './chunk';\nimport { estimateTokens } from './tokens';\nimport { embedChunks } from './embed';\nimport type { BuildOpts } from './config';\n\nexport async function buildBundle(\n  docs: MCXDocument[],\n  opts: boolean | BuildOpts = false\n): Promise<Buffer> {\n  const { zipOriginals, embed } =\n    typeof opts === 'boolean'\n      ? { zipOriginals: opts, embed: undefined }\n      : { zipOriginals: opts.zipOriginals ?? false, embed: opts.embed };\n\n  // Ensure each document has chunks generated.\n  for (const doc of docs) {\n    if (!doc.chunks) {\n      doc.chunks = await chunkText(doc.text);\n    }\n    if (embed) {\n      doc.chunks = await embedChunks(doc.chunks, embed);\n    }\n  }\n\n  // Token statistics.\n  const tokensPerDoc = docs.map(d => estimateTokens(d.text));\n  const tokens = tokensPerDoc.reduce((sum, n) => sum + n, 0);\n\n  // ---- streaming zip via archiver ----\n  const archive = archiver('zip', { zlib: { level: 9 } });\n  const out = new PassThrough();\n  archive.pipe(out);\n  const buffers: Buffer[] = [];\n  out.on('data', chunk => buffers.push(chunk));\n\n  const mixBuf = Buffer.from(\n    JSON.stringify(\n      {\n        version: '0.1.0',\n        created: new Date().toISOString(),\n        tokens,\n        tokensPerDoc,\n        documents: docs\n      },\n      null,\n      2\n    )\n  );\n\n  // Store mixcontext.json uncompressed for deterministic size.\n  archive.append(mixBuf, { name: 'mixcontext.json', store: true });\n\n  if (zipOriginals) {\n    for (const doc of docs) {\n      const p = (doc as any).originalPath as string | undefined;\n      if (p) {\n        archive.file(p, { name: `originals/${doc.filename}` });\n      } else if ((doc as any).original instanceof Buffer) {\n        archive.append((doc as any).original, { name: `originals/${doc.filename}` });\n      }\n    }\n  }\n\n  await archive.finalize();\n\n  return Buffer.concat(buffers);\n}\n","// Copyright 2025 MixContext.ai\n// SPDX-License-Identifier: MIT\n\nimport { GPTTokens } from 'gpt-tokens';\n\n// Default model used for token estimation across the code-base.\nexport const TOKENS_MODEL = 'gpt-3.5-turbo';\n\n/**\n * Estimate token count for a given text using gpt-tokens.\n * Falls back to an upper-bound approximation if tokenizer throws.\n */\nexport function estimateTokens(text: string): number {\n  try {\n    return GPTTokens.contentUsedTokens(TOKENS_MODEL, text);\n  } catch {\n    // Fallback to previous heuristic to avoid runtime breakage in edge cases.\n    return Math.ceil(text.length / 4.2);\n  }\n}\n","export interface OpenAIProviderOpts {\n  model: string;\n  apiKey: string;\n}\n\ninterface OpenAIEmbeddingResponse {\n  data: { embedding: number[] }[];\n}\n\n/**\n * Calls OpenAI embeddings API and returns the embedding vectors for the given input texts.\n */\nexport async function embed(\n  inputs: string[],\n  opts: OpenAIProviderOpts\n): Promise<number[][]> {\n  // When running under tests we shortcut the network call by returning dummy vectors.\n  if (process.env.MIXCONTEXT_MOCK_EMBED === '1') {\n    return inputs.map(() => Array(5).fill(0));\n  }\n\n  const res = await fetch('https://api.openai.com/v1/embeddings', {\n    method: 'POST',\n    headers: {\n      'Content-Type': 'application/json',\n      Authorization: `Bearer ${opts.apiKey}`\n    },\n    body: JSON.stringify({ model: opts.model, input: inputs })\n  });\n\n  if (!res.ok) {\n    const errText = await res.text().catch(() => '');\n    throw new Error(`OpenAI embeddings request failed (${res.status}): ${errText}`);\n  }\n\n  const json = (await res.json()) as OpenAIEmbeddingResponse;\n  return json.data.map(d => d.embedding);\n}\n","export interface LocalProviderOpts {\n  model: string;\n  endpoint: string;\n}\n\ninterface LocalEmbeddingResponse {\n  data: { embedding: number[] }[];\n}\n\n/**\n * Calls a local embedding HTTP endpoint compatible with OpenAI's `/embeddings` schema.\n * The `endpoint` should be the base URL (e.g. http://localhost:11434 or http://127.0.0.1:8080).\n */\nexport async function embed(\n  inputs: string[],\n  opts: LocalProviderOpts\n): Promise<number[][]> {\n  if (process.env.MIXCONTEXT_MOCK_EMBED === '1') {\n    return inputs.map(() => Array(5).fill(0));\n  }\n  const url = `${opts.endpoint.replace(/\\/+$/, '')}/embeddings`;\n  const res = await fetch(url, {\n    method: 'POST',\n    headers: {\n      'Content-Type': 'application/json'\n    },\n    body: JSON.stringify({ model: opts.model, input: inputs })\n  });\n\n  if (!res.ok) {\n    const errText = await res.text().catch(() => '');\n    throw new Error(`Local embeddings request failed (${res.status}): ${errText}`);\n  }\n\n  const json = (await res.json()) as LocalEmbeddingResponse;\n  return json.data.map(d => d.embedding);\n}\n","import { MCXChunk } from '../types';\n\n// Provider implementations (added in ./providers/*)\nimport { embed as openaiEmbed } from './providers/openai';\nimport { embed as localEmbed } from './providers/local';\n\n/**\n * Maximum number of texts that will be sent to the embedding provider in a single request.\n */\nconst BATCH_SIZE = 100;\n\n/**\n * Maximum number of parallel embedding requests that may be in‐flight at any time.\n */\nconst MAX_CONCURRENCY = 4;\n\n/**\n * Maximum number of retries when a request fails with a retry‐able status code.\n */\nconst MAX_RETRIES = 3;\n\n/**\n * Base delay (in milliseconds) that will be multiplied exponentially between retries.\n */\nconst BASE_DELAY_MS = 250;\n\n/**\n * Allowed OpenAI embedding model identifiers.\n */\nexport type OpenAIModels = 'text-embedding-3-small' | 'text-embedding-3-large';\n\nexport interface EmbedOpts {\n  /** Service that will be contacted to create embeddings. */\n  provider: 'openai' | 'local';\n  /**\n   * Name of the embedding model. If omitted a sensible default will be chosen\n   * based on the selected provider.\n   */\n  model?: OpenAIModels | string;\n  /** API key for OpenAI requests. Required when provider === 'openai'. */\n  apiKey?: string;\n  /** Base URL of the local embedding service. Required when provider === 'local'. */\n  endpoint?: string;\n}\n\n/** Utility that pauses execution for the specified number of milliseconds. */\nfunction sleep(ms: number) {\n  return new Promise(resolve => setTimeout(resolve, ms));\n}\n\n/**\n * Executes `fn` with retry logic for responses that indicate the request\n * should be attempted again. Only 429 (rate limit) and 5xx statuses will be\n * retried.\n */\nasync function withRetry<T>(fn: () => Promise<T>): Promise<T> {\n  for (let attempt = 0; attempt <= MAX_RETRIES; attempt++) {\n    try {\n      return await fn();\n    } catch (err) {\n      // Attempt to extract status code from error message of the form \"(...):\"\n      let status = 0;\n      if (err instanceof Error) {\n        const match = err.message.match(/\\((\\d{3})\\)/);\n        if (match) status = Number(match[1]);\n      }\n\n      const isRetryable = status === 429 || (status >= 500 && status < 600);\n      if (!isRetryable || attempt === MAX_RETRIES) {\n        throw err;\n      }\n\n      const delay = BASE_DELAY_MS * Math.pow(2, attempt); // exponential backoff\n      /* istanbul ignore next */\n      await sleep(delay);\n    }\n  }\n\n  // This point should be unreachable but satisfies TypeScript return expectations\n  /* istanbul ignore next */\n  throw new Error('withRetry: exhausted retries without throwing');\n}\n\n/**\n * Obtain embeddings for the provided chunks using the configured provider.\n * This mutates the returned array by attaching an `embedding` property to\n * each chunk.\n */\nexport async function embedChunks(\n  chunks: MCXChunk[],\n  opts: EmbedOpts\n): Promise<MCXChunk[]> {\n  if (!opts) throw new Error('embedChunks: `opts` must be provided');\n\n  const texts = chunks.map(c => c.text);\n\n  // Pre‐allocate array that will be filled with embedding vectors in order.\n  const combinedVectors: number[][] = Array(texts.length);\n\n  // Split texts into fixed‐size batches while tracking their start index.\n  const tasks: { slice: string[]; startIdx: number }[] = [];\n  for (let i = 0; i < texts.length; i += BATCH_SIZE) {\n    tasks.push({ slice: texts.slice(i, i + BATCH_SIZE), startIdx: i });\n  }\n\n  // Worker routine that processes tasks sequentially but multiple workers run concurrently.\n  let taskCursor = 0;\n  async function worker() {\n    while (true) {\n      const myIndex = taskCursor++;\n      if (myIndex >= tasks.length) break;\n\n      const { slice, startIdx } = tasks[myIndex];\n\n      const vectors = await withRetry(() => {\n        switch (opts.provider) {\n          case 'openai': {\n            if (!opts.apiKey) {\n              throw new Error('OpenAI embedding requires `apiKey`');\n            }\n            const model = opts.model ?? 'text-embedding-3-small';\n            return openaiEmbed(slice, { model, apiKey: opts.apiKey });\n          }\n          case 'local': {\n            if (!opts.endpoint) {\n              throw new Error('Local embedding requires `endpoint`');\n            }\n            const model = opts.model ?? 'text-embedding-3-small';\n            return localEmbed(slice, { model, endpoint: opts.endpoint });\n          }\n          /* istanbul ignore next */\n          default: {\n            const neverCheck: never = opts.provider;\n            throw new Error(`Unsupported embedding provider: ${neverCheck}`);\n          }\n        }\n      });\n\n      // Insert vectors into the pre‐allocated result array.\n      for (let i = 0; i < vectors.length; i++) {\n        combinedVectors[startIdx + i] = vectors[i];\n      }\n    }\n  }\n\n  // Spin up workers respecting MAX_CONCURRENCY.\n  const workerCount = Math.min(MAX_CONCURRENCY, tasks.length || 1);\n  const workers = Array.from({ length: workerCount }, () => worker());\n  await Promise.all(workers);\n\n  return chunks.map((c, i) => ({\n    ...c,\n    embedding: combinedVectors[i]\n  }));\n}\n","// File: src/vector/local.ts – local vector DB implementation\n\nimport { toSql, fromSql } from 'pgvector';\nimport * as sqliteVec from 'sqlite-vec';\n\n// Attempt to load the native better-sqlite3 module. If it cannot be required\n// *or* instantiating it fails (e.g. bindings missing) we fall back to a very\n// small in-memory stub that provides exactly the subset of the API used by\n// LocalVectorDB. This guarantees that the rest of the package – including the\n// test-suite and CLI – still works even when the native addon cannot be\n// compiled (for example on unsupported platforms or when post-install scripts\n// are disabled).\n\nlet BetterSqlite: any;\n\n// --- Minimal JS stub ---------------------------------------------------------\nclass InMemorySQLite {\n  private _rows: Array<{ id: string; vector: Buffer; meta: string } > = [];\n\n\n  constructor(_filename: string = ':memory:') {}\n\n\n  /** No-op for the subset of SQL that creates tables / indexes. */\n  exec(_sql: string): void {}\n\n  /** Return a 'statement' object whose behaviour depends on the SQL string. */\n  prepare(sql: string) {\n    // Basic detection – we only look at the beginning of the statement which\n    // is good enough for the handful of different queries issued below.\n    const upper = sql.trim().toUpperCase();\n\n    // INSERT / UPSERT implementation – stores or replaces the row.\n    if (upper.startsWith('INSERT')) {\n      return {\n        run: (id: string, vecBuf: Buffer, metaJson: string) => {\n          const idx = this._rows.findIndex(r => r.id === id);\n          const row = { id, vector: vecBuf, meta: metaJson };\n          if (idx >= 0) this._rows[idx] = row; else this._rows.push(row);\n        }\n      } as any;\n    }\n\n    // SELECT implementation – returns a shallow copy of all rows. For the\n    // purposes of LocalVectorDB we never filter at the SQL level when the JS\n    // fallback is active, so returning everything is sufficient.\n    if (upper.startsWith('SELECT')) {\n      return {\n        all: () => this._rows.slice()\n      } as any;\n    }\n\n    // Default dummy implementation (covers CREATE TABLE / INDEX, etc.).\n    return {\n      run: () => {},\n      all: () => []\n    } as any;\n  }\n}\n\ntry {\n  // eslint-disable-next-line\n  BetterSqlite = require('better-sqlite3');\n} catch {\n  BetterSqlite = InMemorySQLite;\n}\n\nexport interface VecOptions {\n  /** Optional path of the SQLite database file. Pass `:memory:` for in-memory DB. */\n  dbPath?: string;\n  /** Dimensionality (number of elements) of every embedding vector. */\n  dim: number;\n}\n\n/** Lightweight vector database implementation that stores embeddings in a local\n *  SQLite database (better-sqlite3) with pgvector binary representation.\n *\n *  When the optional `sqlite-vec` extension is available it will be loaded so\n *  that cosine distance can be computed directly inside SQL. If the extension\n *  cannot be loaded (e.g. during CI where loadable extensions are disabled)\n *  the driver transparently falls back to an in-memory JavaScript distance\n *  computation.\n *\n *  Embeddings are stored in a table called `embeddings` using the following\n *  schema:\n *    id     TEXT PRIMARY KEY\n *    vector BLOB NOT NULL  -- pgvector binary format (4 + 4*dim bytes)\n *    meta   TEXT           -- arbitrary JSON user metadata\n */\nexport class LocalVectorDB {\n  private db: any; // Using `any` to avoid importing the entire better-sqlite3 types\n  private readonly dim: number;\n  private readonly jsFallback: boolean;\n\n  constructor(opts: VecOptions) {\n    if (!opts || typeof opts.dim !== 'number' || opts.dim <= 0) {\n      throw new Error('LocalVectorDB: `dim` must be a positive integer');\n    }\n\n    // Some environments (e.g. CI, sandboxed package managers) may be able to\n    // import the better-sqlite3 module but still fail when *instantiating* the\n    // Database because the native bindings could not be located. To cover this\n    // case we try/catch the constructor and fall back to the JS stub.\n    try {\n      this.db = new BetterSqlite(opts.dbPath || ':memory:');\n    } catch {\n      // Recreate using the in-memory stub defined above.\n      this.db = new InMemorySQLite();\n      // When we are using the stub we must always run JS-side similarity.\n      this.jsFallback = true as any; // will be overwritten below but helps TS\n    }\n    this.dim = opts.dim;\n\n    // Try to load the optional sqlite-vec extension that registers cosine_distance()\n    let fallback = false;\n    try {\n      // sqlite-vec exports a convenience helper that registers all SQL\n      // functions on the provided Database instance.\n\n      // @ts-ignore – runtime only import\n      sqliteVec.load(this.db);\n    } catch {\n      fallback = true; // module missing or failed to load, use JS fallback\n    }\n    this.jsFallback = fallback;\n\n    // Create table if it does not exist.\n    const colsSql = `id TEXT PRIMARY KEY, vector BLOB NOT NULL, meta TEXT`;\n    this.db.exec(`CREATE TABLE IF NOT EXISTS embeddings (${colsSql})`);\n\n    // When the sqlite-vec extension is available create an index so that KNN\n    // queries are efficient. The `IF NOT EXISTS` clause will no-op when the\n    // extension is missing.\n    try {\n      this.db.exec(`CREATE INDEX IF NOT EXISTS idx_embeddings_vector ON embeddings(vector)`);\n    } catch {\n      /* ignore */\n    }\n  }\n\n  /** Insert or replace a vector together with user metadata. */\n  upsert(id: string, vector: number[], meta: any = {}): void {\n    if (vector.length !== this.dim) {\n      throw new Error(`Vector dimensionality mismatch (expected ${this.dim}, got ${vector.length})`);\n    }\n    const buf: Buffer = toSql(vector);\n    const json = JSON.stringify(meta ?? {});\n    const stmt = this.db.prepare(\n      `INSERT INTO embeddings (id, vector, meta) VALUES (?, ?, ?)\n       ON CONFLICT(id) DO UPDATE SET vector = excluded.vector, meta = excluded.meta`\n    );\n    stmt.run(id, buf, json);\n  }\n\n  /** Find the `k` vectors nearest to `query` using cosine similarity.  */\n  search(query: number[], k = 8): Array<{ id: string; score: number; meta: any }> {\n    if (query.length !== this.dim) {\n      throw new Error(`Query dimensionality mismatch (expected ${this.dim}, got ${query.length})`);\n    }\n\n    // If sqlite-vec is available we can perform the computation inside SQL using\n    // the cosine_distance() scalar function which is written in WASM and very\n    // fast. Otherwise we fall back to JS distance calculation.\n    if (!this.jsFallback) {\n      const buf = toSql(query);\n      const rows: any[] = this.db\n        .prepare(\n          `SELECT id, vector, meta, cosine_distance(vector, ?) AS dist\n           FROM embeddings\n           ORDER BY dist ASC\n           LIMIT ?`\n        )\n        .all(buf, k);\n\n      // If the sqlite-vec extension is active each row should include a valid\n      // numeric `dist` column. However in test environments we mock the\n      // database driver and do not actually compute the distance, leaving the\n      // property undefined. In that case we fall back to a JS similarity\n      // calculation so the semantics remain unchanged.\n      const haveValidDist = rows.length > 0 && typeof rows[0].dist === 'number' && !Number.isNaN(rows[0].dist);\n\n      if (haveValidDist) {\n        return rows.map(r => ({\n          id: r.id as string,\n          score: 1 - (r.dist as number), // convert distance → similarity\n          meta: r.meta ? JSON.parse(r.meta as string) : undefined\n        }));\n      }\n\n      // Fall through to JS path when dist missing / invalid\n    }\n\n    // --- JS fallback path (no sqlite-vec or dist missing) ---\n    const all: any[] = this.db.prepare(`SELECT id, vector, meta FROM embeddings`).all();\n    const results = all.map(row => {\n      const vec: number[] = fromSql(row.vector as Buffer);\n      const sim = this.cosineSimilarity(query, vec);\n      return { id: row.id as string, score: sim, meta: row.meta ? JSON.parse(row.meta as string) : undefined };\n    });\n\n    results.sort((a, b) => b.score - a.score); // descending similarity\n    return results.slice(0, k);\n  }\n\n  /** Pure JS cosine similarity – avoids extra deps when sqlite-vec unavailable. */\n  private cosineSimilarity(a: number[], b: number[]): number {\n    let dot = 0;\n    let magA = 0;\n    let magB = 0;\n    for (let i = 0; i < a.length; i++) {\n      dot += a[i] * b[i];\n      magA += a[i] * a[i];\n      magB += b[i] * b[i];\n    }\n    if (magA === 0 || magB === 0) return 0;\n    return dot / (Math.sqrt(magA) * Math.sqrt(magB));\n  }\n}\n\n\n\n\n\n\n\n\n"],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;;AACA,uBAAwB;AACxB,uBAAe;AACf,gBAA+B;AAC/B,kBAAiB;AACjB,mBAAgC;AAEhC,0BAAwB;;;ACHxB,kBAA2B;;;ACJ3B,eAAsB,SAAS,MAA+B;AAE5D,MAAI,OAAO,WAAW,QAAQ,QAAQ,WAAW,YAAY;AAC3D,UAAM,UAAU,IAAI,YAAY;AAChC,UAAM,OAAO,QAAQ,OAAO,IAAI;AAChC,UAAM,UAAU,MAAM,WAAW,OAAO,OAAO,OAAO,WAAW,IAAI;AAErE,UAAM,YAAY,MAAM,KAAK,IAAI,WAAW,OAAO,CAAC;AACpD,WAAO,UAAU,IAAI,OAAK,EAAE,SAAS,EAAE,EAAE,SAAS,GAAG,GAAG,CAAC,EAAE,KAAK,EAAE;AAAA,EACpE;AAIA,QAAM,EAAE,WAAW,IAAI,MAAM,OAAO,QAAa;AACjD,SAAO,WAAW,QAAQ,EAAE,OAAO,IAAI,EAAE,OAAO,KAAK;AACvD;;;ACJA,eAAsB,UACpB,MACA,YAAY,KACZ,OAA6B,CAAC,GACT;AACrB,QAAM,WAAqB,CAAC;AAE5B,MAAI,OAAQ,MAAc,cAAc,YAAY;AAClD,UAAM,WAAY,KAAa;AAC/B,UAAM,MAAM,IAAI,SAAS,MAAM,EAAE,aAAa,WAAW,CAAC;AAC1D,eAAW,EAAE,QAAQ,KAAK,IAAI,QAAQ,IAAI,GAAG;AAC3C,eAAS,KAAK,OAAO;AAAA,IACvB;AAAA,EACF,OAAO;AAEL,aAAS,KAAK,GAAG,KAAK,MAAM,eAAe,CAAC;AAAA,EAC9C;AAEA,QAAM,SAAqB,CAAC;AAC5B,MAAI,SAAS;AACb,MAAI,OAAO;AAEX,QAAM,OAAO,MAAM;AACjB,QAAI,OAAO,KAAK,EAAE,QAAQ;AACxB,aAAO,KAAK,EAAE,KAAK,OAAO,QAAQ,MAAM,OAAO,KAAK,EAAE,CAAC;AACvD,eAAS;AACT;AAAA,IACF;AAAA,EACF;AAEA,aAAW,YAAY,UAAU;AAC/B,QAAI,OAAO,SAAS,SAAS,UAAU,WAAW;AAChD,iBAAW,SAAS,MAAM,MAAM,SAAS,KAAK;AAAA,IAChD,OAAO;AACL,WAAK;AAEL,UAAI,SAAS,SAAS,WAAW;AAC/B,iBAAS,IAAI,GAAG,IAAI,SAAS,QAAQ,KAAK,WAAW;AACnD,mBAAS,SAAS,MAAM,GAAG,IAAI,SAAS;AACxC,eAAK;AAAA,QACP;AAAA,MACF,OAAO;AACL,iBAAS,SAAS,KAAK;AAAA,MACzB;AAAA,IACF;AAAA,EACF;AACA,OAAK;AAGL,MAAI,KAAK,WAAW,OAAO;AACzB,UAAM,OAAO,oBAAI,IAAoB;AACrC,eAAW,KAAK,QAAQ;AACtB,UAAI,CAAC,EAAE,KAAM;AACb,YAAM,OAAO,MAAM,SAAS,EAAE,IAAI;AAClC,YAAM,WAAW,KAAK,IAAI,IAAI;AAC9B,UAAI,aAAa,QAAW;AAE1B,QAAC,EAAU,QAAQ;AACnB,UAAE,OAAO;AAAA,MACX,OAAO;AACL,aAAK,IAAI,MAAM,EAAE,GAAG;AAAA,MACtB;AAAA,IACF;AAAA,EACF;AAEA,SAAO;AACT;;;AFtEA,iBAA4B;;;AGM5B,IAAM,UAA0B,CAAC;AAY1B,SAAS,UAAUA,OAAwC;AAChE,SAAO,QAAQ;AAAA,IAAK,CAAC,MACnB,OAAO,EAAE,SAAS,WAAW,EAAE,SAASA,QAAO,EAAE,KAAK,KAAKA,KAAI;AAAA,EACjE;AACF;;;ACzBA,IAAM,WAAW,oBAAI,IAAsB;AAEpC,SAAS,eAAeC,OAAc,IAAoB;AAC/D,WAAS,IAAIA,MAAK,YAAY,GAAG,EAAE;AACrC;AAEO,SAAS,UAAUA,OAAoC;AAE5D,QAAM,WAAW,SAAS,IAAIA,MAAK,YAAY,CAAC;AAChD,MAAI,SAAU,QAAO;AAGrB,QAAM,SAAS,UAAUA,KAAI;AAC7B,SAAO,QAAQ;AACjB;;;AJTA,qBAAe;AACf,yBAAmB;AAGnB,eAAsB,SAAS,OAAoB,QAA2B,CAAC,GAAyB;AAGtG,MAAI;AACJ,MAAI,eAAe;AAEnB,MAAI,OAAO,UAAU,UAAU;AAC7B,UAAM,OAAO,MAAM,eAAAC,QAAG,SAAS,KAAK,KAAK;AACzC,mBAAe,KAAK;AACpB,UAAM,UAAO,wBAAoB;AAAA,MAC/B,KAAK;AAAA,MACL,eAAe;AAAA,MACf,gBAAgB;AAAA,IAClB,CAAC,EAAE;AAAA,EACL,OAAO;AACL,UAAM,MAAM;AACZ,mBAAe,IAAI;AACnB,UAAM,YAAY,IAAI,WAAW,IAAI,QAAQ,IAAI,YAAY,IAAI,UAAU;AAC3E,UAAM,UAAO,wBAAoB;AAAA,MAC/B,MAAM;AAAA,MACN,eAAe;AAAA,MACf,gBAAgB;AAAA,IAClB,CAAC,EAAE;AAAA,EACL;AAEA,QAAM,QAAkB,CAAC;AACzB,WAAS,UAAU,GAAG,WAAW,IAAI,UAAU,WAAW;AACxD,UAAM,OAAO,MAAM,IAAI,QAAQ,OAAO;AACtC,UAAM,cAAc,MAAM,KAAK,eAAe;AAE9C,UAAM,WAAW,YAAY,MAC1B,IAAI,CAAC,MAAW,EAAE,GAAa,EAC/B,KAAK,GAAG;AACX,UAAM,KAAK,SAAS,KAAK,CAAC;AAAA,EAC5B;AAEA,QAAM,OAAO,MAAM,KAAK,MAAM;AAE9B,QAAM,MAAkE;AAAA,IACtE,QAAI,YAAAC,IAAK;AAAA,IACT,UAAU;AAAA,IACV,UAAU;AAAA,IACV;AAAA,IACA,QAAQ,MAAM,UAAU,IAAI;AAAA,IAC5B,MAAM,EAAE,OAAO,IAAI,UAAU,aAAa;AAAA,EAC5C;AAEA,MAAI,OAAO,UAAU,UAAU;AAC7B,QAAI,eAAe;AAGnB,QAAI;AACF,YAAM,OAAO,mBAAAC,QAAO,WAAW,QAAQ;AACvC,YAAM,IAAI,QAAc,CAAC,SAAS,WAAW;AAC3C,cAAM,KAAK,eAAAF,QAAG,iBAAiB,KAAK;AACpC,WAAG,GAAG,QAAQ,WAAS,KAAK,OAAO,KAAK,CAAC;AACzC,WAAG,GAAG,SAAS,MAAM;AACrB,WAAG,GAAG,OAAO,MAAM;AACjB,cAAI,OAAO,EAAE,GAAI,IAAI,QAAQ,CAAC,GAAI,QAAQ,KAAK,OAAO,KAAK,EAAE;AAC7D,kBAAQ;AAAA,QACV,CAAC;AAAA,MACH,CAAC;AAAA,IACH,QAAQ;AAAA,IAAgB;AAAA,EAC1B,OAAO;AACL,QAAI,WAAW;AAAA,EACjB;AACA,SAAO;AACT;AAGA,eAAe,mBAAmB,QAAQ;;;AK/E1C,IAAAG,eAA2B;AAC3B,qBAAoB;AAEpB,IAAAC,kBAAe;AACf,sBAAqB;AACrB,iBAAgB;AAGhB,eAAsB,UAAU,OAA0C;AACxE,MAAI,OAAO;AACX,MAAI,eAAe;AAEnB,MAAI,OAAO,UAAU,UAAU;AAC7B,UAAM,OAAO,MAAM,gBAAAC,QAAG,SAAS,KAAK,KAAK;AACzC,mBAAe,KAAK;AAGpB,UAAM,KAAK,gBAAAA,QAAG,iBAAiB,KAAK;AACpC,UAAM,YAAY,GAAG,KAAK,gBAAAC,QAAS,SAAS,oBAAoB,CAAC;AAEjE,WAAO,MAAM,2BAA2B,SAAS;AAAA,EACnD,OAAO;AACL,UAAM,MAAM;AACZ,mBAAe,IAAI;AACnB,UAAM,SAAS,MAAM,eAAAC,QAAQ,eAAe,EAAE,QAAQ,IAAI,CAAC;AAC3D,WAAO,OAAO,MAAM,KAAK;AAAA,EAC3B;AAEA,QAAM,MAAkE;AAAA,IACtE,QAAI,aAAAC,IAAK;AAAA,IACT,UAAU;AAAA,IACV,UAAU;AAAA,IACV;AAAA,IACA,QAAQ,MAAM,UAAU,IAAI;AAAA,IAC5B,MAAM,EAAE,aAAa;AAAA,EACvB;AAEA,MAAI,OAAO,UAAU,UAAU;AAC7B,QAAI,eAAe;AAAA,EACrB,OAAO;AACL,QAAI,WAAW;AAAA,EACjB;AAEA,SAAO;AACT;AAEA,eAAe,2BAA2B,QAAgD;AACxF,SAAO,IAAI,QAAgB,CAAC,SAAS,WAAW;AAC9C,UAAM,SAAS,WAAAC,QAAI,aAAa,MAAM,CAAC,CAAC;AACxC,QAAI,YAAsB,CAAC;AAC3B,WAAO,GAAG,QAAQ,CAAC,MAAc;AAC/B,gBAAU,KAAK,CAAC;AAAA,IAClB,CAAC;AACD,WAAO,GAAG,SAAS,MAAM;AACzB,WAAO,GAAG,OAAO,MAAM;AACrB,cAAQ,UAAU,KAAK,GAAG,EAAE,QAAQ,QAAQ,GAAG,EAAE,KAAK,CAAC;AAAA,IACzD,CAAC;AACD,WAAO,KAAK,MAAM;AAAA,EACpB,CAAC;AACH;AAGA,eAAe,2EAA2E,SAAS;;;AC9DnG,IAAAC,eAA2B;AAC3B,+BAAgC;AAGhC,qBAAoB;AACpB,wBAAkB;AAClB,uBAAsB;AACtB,IAAAC,kBAA+B;AAK/B,eAAsB,UAAU,OAA0C;AAExE,QAAM,MACJ,OAAO,UAAU,WAAW,MAAM,gBAAAC,SAAG,SAAS,KAAK,IAAK;AAG1D,MAAI,WAAW;AACf,MAAI;AACF,UAAM,WAAW,eAAAC,QAAQ,OAAO,GAAG;AACnC,QAAI,OAAO,aAAa,YAAY,UAAU;AAC5C,iBAAW,SAAS,YAAY;AAAA,IAClC;AAAA,EACF,QAAQ;AAAA,EAER;AAEA,MAAI;AACJ,MAAI;AACF,WAAO,kBAAAC,QAAM,OAAO,KAAK,QAAQ;AAAA,EACnC,QAAQ;AAEN,WAAO,IAAI,SAAS,MAAM;AAAA,EAC5B;AAIA,MAAI,QAAQ;AACZ,MAAI,UAAU;AACd,MAAI;AAEJ,MAAI;AACF,UAAM,SAAS,UAAM,0CAAgB,IAAI;AAEzC,QAAI,QAAQ;AACV,cAAQ,OAAO,SAAS;AACxB,iBAAW,OAAO,WAAW,IAAI,KAAK;AACtC,UAAI,OAAO,OAAQ,QAAO,OAAO;AAAA,IACnC;AAAA,EACF,QAAQ;AAAA,EAER;AAEA,MAAI,CAAC,SAAS;AACZ,cAAU,KAAK,QAAQ,YAAY,GAAG;AAAA,EACxC;AAEA,QAAM,OAAO,QAAQ,QAAQ,QAAQ,GAAG,EAAE,KAAK;AAG/C,MAAI;AACJ,MAAI;AACF,UAAM,YAAQ,wBAAM,KAAK,MAAM,GAAG,GAAM,GAAG,EAAE,WAAW,GAAG,CAAC;AAC5D,QAAI,SAAS,UAAU,MAAO,QAAO;AAAA,EACvC,QAAQ;AAAA,EAER;AAEA,QAAM,OAA4B,EAAE,MAAM;AAC1C,MAAI,KAAM,MAAK,OAAO;AACtB,MAAI,KAAM,MAAK,OAAO;AAEtB,QAAM,MAAmB;AAAA,IACvB,QAAI,aAAAC,IAAK;AAAA,IACT,UAAU;AAAA,IACV,UAAU;AAAA,IACV;AAAA,IACA,QAAQ,MAAM,UAAU,IAAI;AAAA,IAC5B;AAAA,EACF;AAEA,SAAO;AACT;AAEA,eAAe,aAAa,SAAS;AACrC,eAAe,yBAAyB,SAAS;;;ACtFjD,IAAAC,eAA2B;AAK3B,uBAA6D;AAG7D,IAAI,SAAiC;AAKrC,eAAe,aAAuC;AACpD,MAAI,OAAQ,QAAO;AACnB,WAAS,UAAM,+BAAa,KAAK;AACjC,SAAO;AACT;AAOA,eAAsB,WACpB,KACA,OAA0B,CAAC,GAC3B,QACsB;AAItB,MAAI,CAAC,KAAK,KAAK;AACb,WAAO;AAAA,MACL,QAAI,aAAAC,IAAK;AAAA,MACT,UAAU;AAAA,MACV,UAAU;AAAA,MACV,MAAM;AAAA,MACN,MAAM,EAAE,KAAK,MAAM;AAAA,IACrB;AAAA,EACF;AAEA,QAAM,IAAI,MAAM,WAAW;AAG3B,QAAM,EAAE,KAAK,IAAI,MAAO,EAAU,UAAU,KAAK;AAAA,IAC/C,QAAQ,CAAC,MAAW;AAClB,UAAI,OAAQ,SAAQ,KAAK,YAAY,CAAC;AAAA,IACxC;AAAA,EACF,CAAQ;AAER,QAAM,OAAO,KAAK,KAAK,KAAK;AAC5B,QAAM,QAAQ,KAAK,MAAM,OAAO,EAAE,OAAO,OAAO,EAAE;AAElD,QAAM,MAAmB;AAAA,IACvB,QAAI,aAAAA,IAAK;AAAA,IACT,UAAU;AAAA,IACV,UAAU;AAAA,IACV;AAAA,IACA,QAAQ,MAAM,UAAU,IAAI;AAAA,IAC5B,MAAM;AAAA,MACJ,KAAK;AAAA,MACL;AAAA,MACA,YAAY,KAAK;AAAA,IACnB;AAAA,EACF;AAEA,SAAO;AACT;AAGA,eAAe,aAAa,UAAiB;AAC7C,eAAe,cAAc,UAAiB;;;ACrE9C,uBAAmC;;;ACgC5B,IAAM,uBAAN,cAAmC,MAAM;AAAA,EAE9C,YAAYC,OAAc,UAAU,yBAAyB;AAC3D,UAAM,OAAO;AACb,SAAK,OAAO;AACZ,SAAK,OAAOA;AAAA,EACd;AACF;;;ADnCA,IAAAC,kBAA+B;AAC/B,uBAAiB;AACjB,WAAsB;AAItB,IAAM,mBAAmB,KAAK,OAAO;AAErC,eAAsB,UACpB,OACA,QAAmB,CAAC,GACE;AAEtB,MAAI,OAAO,UAAU,UAAU;AAC7B,UAAM,UAAU,iBAAAC,QAAK,QAAQ,KAAK;AAClC,UAAM,YAAiB,YAAO,OAAO,KAAK;AAC1C,UAAMC,UAAS,UAAU,SAAmB;AAC5C,QAAI,CAACA,QAAQ,OAAM,IAAI,qBAAqB,qBAAqB,SAAS,EAAE;AAC5E,UAAMC,OAAM,MAAMD,QAAO,SAAS,KAAK;AACvC,IAACC,KAAY,eAAe;AAC5B,WAAOA;AAAA,EACT;AAIA,MAAI;AACJ,MAAI,iBAAiB,QAAQ;AAC3B,UAAM;AAAA,EACR,OAAO;AAEL,UAAM,WAAgB;AACtB,QAAI,SAAS,QAAQ,kBAAkB;AAErC,YAAM,EAAE,MAAM,SAAS,QAAQ,IAAI,MAAM,aAAa,MAAM,SAAS,YAAY,CAAC;AAClF,YAAM,eAAe,SAAS,QAAa,YAAO,SAAS,IAAI,KAAK;AACpE,YAAMD,UAAS,UAAU,YAAsB;AAC/C,UAAI,CAACA,QAAQ,OAAM,IAAI,qBAAqB,qBAAqB,YAAY,EAAE;AAC/E,YAAMC,OAAM,MAAMD,QAAO,SAAS,KAAK;AACvC,MAACC,KAAY,eAAe;AAE5B,MAACA,KAAY,cAAc;AAC3B,aAAOA;AAAA,IACT;AACA,UAAM,OAAO,KAAK,MAAM,SAAS,YAAY,CAAC;AAAA,EAChD;AAGA,MAAI,IAAI,UAAU,kBAAkB;AAClC,UAAM,EAAE,MAAM,QAAQ,IAAI,MAAM,aAAa,GAAG;AAChD,WAAO,MAAM,UAAU,SAAS,KAAK;AAAA,EACvC;AAEA,MAAI;AACJ,MAAI;AACF,WAAO,UAAM,qCAAmB,GAAG;AAAA,EACrC,QAAQ;AACN,WAAO;AAAA,EACT;AAEA,MAAI,WAAW,MAAM,SAAS,iBAAiB,OAAQ,MAAc,OAAO;AAC5E,OAAK,aAAa,8BAA8B,aAAa,iBAAiB,IAAI,SAAS,GAAG;AAC5F,UAAM,SAAS,IAAI,MAAM,GAAG,IAAI,EAAE,SAAS,MAAM;AACjD,QAAI,OAAO,SAAS,GAAG,KAAK,OAAO,SAAS,IAAI,KAAK,CAAC,OAAO,SAAS,IAAI,GAAG;AAC3E,iBAAW;AAAA,IACb;AAAA,EACF;AAEA,QAAM,SAAS,UAAU,QAAQ;AACjC,MAAI,CAAC,OAAQ,OAAM,IAAI,qBAAqB,qBAAqB,QAAQ,EAAE;AAE3E,QAAM,MAAmB,MAAM,OAAO,KAAK,KAAK;AAChD,EAAC,IAAY,WAAW;AACxB,MAAI,OAAO,EAAE,GAAI,IAAI,QAAQ,CAAC,GAAI,cAAc,IAAI,OAAO;AAC3D,SAAO;AACT;AAGA,eAAe,aAAa,MAAqF;AAC/G,QAAM,EAAE,SAAS,WAAW,GAAG,IAAI,gBAAAC;AACnC,QAAM,KAAK,MAAM,OAAO,IAAS;AACjC,QAAM,MAAM,MAAM,QAAQ,iBAAAH,QAAK,KAAK,GAAG,OAAO,GAAG,QAAQ,CAAC;AAC1D,QAAM,UAAU,iBAAAA,QAAK,KAAK,KAAK,OAAO;AACtC,QAAM,UAAU,SAAS,gBAAgB,SAAS,OAAO,OAAO,KAAK,IAAI,CAAC;AAC1E,SAAO,EAAE,MAAM,SAAS,SAAS,MAAM,GAAG,KAAK,EAAE,WAAW,MAAM,OAAO,KAAK,CAAC,EAAE;AACnF;;;AE3FA,sBAAqB;AACrB,oBAA4B;;;ACF5B,wBAA0B;AAGnB,IAAM,eAAe;AAMrB,SAAS,eAAe,MAAsB;AACnD,MAAI;AACF,WAAO,4BAAU,kBAAkB,cAAc,IAAI;AAAA,EACvD,QAAQ;AAEN,WAAO,KAAK,KAAK,KAAK,SAAS,GAAG;AAAA,EACpC;AACF;;;ACPA,eAAsB,MACpB,QACA,MACqB;AAErB,MAAI,QAAQ,IAAI,0BAA0B,KAAK;AAC7C,WAAO,OAAO,IAAI,MAAM,MAAM,CAAC,EAAE,KAAK,CAAC,CAAC;AAAA,EAC1C;AAEA,QAAM,MAAM,MAAM,MAAM,wCAAwC;AAAA,IAC9D,QAAQ;AAAA,IACR,SAAS;AAAA,MACP,gBAAgB;AAAA,MAChB,eAAe,UAAU,KAAK,MAAM;AAAA,IACtC;AAAA,IACA,MAAM,KAAK,UAAU,EAAE,OAAO,KAAK,OAAO,OAAO,OAAO,CAAC;AAAA,EAC3D,CAAC;AAED,MAAI,CAAC,IAAI,IAAI;AACX,UAAM,UAAU,MAAM,IAAI,KAAK,EAAE,MAAM,MAAM,EAAE;AAC/C,UAAM,IAAI,MAAM,qCAAqC,IAAI,MAAM,MAAM,OAAO,EAAE;AAAA,EAChF;AAEA,QAAM,OAAQ,MAAM,IAAI,KAAK;AAC7B,SAAO,KAAK,KAAK,IAAI,OAAK,EAAE,SAAS;AACvC;;;ACxBA,eAAsBI,OACpB,QACA,MACqB;AACrB,MAAI,QAAQ,IAAI,0BAA0B,KAAK;AAC7C,WAAO,OAAO,IAAI,MAAM,MAAM,CAAC,EAAE,KAAK,CAAC,CAAC;AAAA,EAC1C;AACA,QAAM,MAAM,GAAG,KAAK,SAAS,QAAQ,QAAQ,EAAE,CAAC;AAChD,QAAM,MAAM,MAAM,MAAM,KAAK;AAAA,IAC3B,QAAQ;AAAA,IACR,SAAS;AAAA,MACP,gBAAgB;AAAA,IAClB;AAAA,IACA,MAAM,KAAK,UAAU,EAAE,OAAO,KAAK,OAAO,OAAO,OAAO,CAAC;AAAA,EAC3D,CAAC;AAED,MAAI,CAAC,IAAI,IAAI;AACX,UAAM,UAAU,MAAM,IAAI,KAAK,EAAE,MAAM,MAAM,EAAE;AAC/C,UAAM,IAAI,MAAM,oCAAoC,IAAI,MAAM,MAAM,OAAO,EAAE;AAAA,EAC/E;AAEA,QAAM,OAAQ,MAAM,IAAI,KAAK;AAC7B,SAAO,KAAK,KAAK,IAAI,OAAK,EAAE,SAAS;AACvC;;;AC3BA,IAAM,aAAa;AAKnB,IAAM,kBAAkB;AAKxB,IAAM,cAAc;AAKpB,IAAM,gBAAgB;AAsBtB,SAAS,MAAM,IAAY;AACzB,SAAO,IAAI,QAAQ,aAAW,WAAW,SAAS,EAAE,CAAC;AACvD;AAOA,eAAe,UAAa,IAAkC;AAC5D,WAAS,UAAU,GAAG,WAAW,aAAa,WAAW;AACvD,QAAI;AACF,aAAO,MAAM,GAAG;AAAA,IAClB,SAAS,KAAK;AAEZ,UAAI,SAAS;AACb,UAAI,eAAe,OAAO;AACxB,cAAM,QAAQ,IAAI,QAAQ,MAAM,aAAa;AAC7C,YAAI,MAAO,UAAS,OAAO,MAAM,CAAC,CAAC;AAAA,MACrC;AAEA,YAAM,cAAc,WAAW,OAAQ,UAAU,OAAO,SAAS;AACjE,UAAI,CAAC,eAAe,YAAY,aAAa;AAC3C,cAAM;AAAA,MACR;AAEA,YAAM,QAAQ,gBAAgB,KAAK,IAAI,GAAG,OAAO;AAEjD,YAAM,MAAM,KAAK;AAAA,IACnB;AAAA,EACF;AAIA,QAAM,IAAI,MAAM,+CAA+C;AACjE;AAOA,eAAsB,YACpB,QACA,MACqB;AACrB,MAAI,CAAC,KAAM,OAAM,IAAI,MAAM,sCAAsC;AAEjE,QAAM,QAAQ,OAAO,IAAI,OAAK,EAAE,IAAI;AAGpC,QAAM,kBAA8B,MAAM,MAAM,MAAM;AAGtD,QAAM,QAAiD,CAAC;AACxD,WAAS,IAAI,GAAG,IAAI,MAAM,QAAQ,KAAK,YAAY;AACjD,UAAM,KAAK,EAAE,OAAO,MAAM,MAAM,GAAG,IAAI,UAAU,GAAG,UAAU,EAAE,CAAC;AAAA,EACnE;AAGA,MAAI,aAAa;AACjB,iBAAeC,UAAS;AACtB,WAAO,MAAM;AACX,YAAM,UAAU;AAChB,UAAI,WAAW,MAAM,OAAQ;AAE7B,YAAM,EAAE,OAAO,SAAS,IAAI,MAAM,OAAO;AAEzC,YAAM,UAAU,MAAM,UAAU,MAAM;AACpC,gBAAQ,KAAK,UAAU;AAAA,UACrB,KAAK,UAAU;AACb,gBAAI,CAAC,KAAK,QAAQ;AAChB,oBAAM,IAAI,MAAM,oCAAoC;AAAA,YACtD;AACA,kBAAM,QAAQ,KAAK,SAAS;AAC5B,mBAAO,MAAY,OAAO,EAAE,OAAO,QAAQ,KAAK,OAAO,CAAC;AAAA,UAC1D;AAAA,UACA,KAAK,SAAS;AACZ,gBAAI,CAAC,KAAK,UAAU;AAClB,oBAAM,IAAI,MAAM,qCAAqC;AAAA,YACvD;AACA,kBAAM,QAAQ,KAAK,SAAS;AAC5B,mBAAOC,OAAW,OAAO,EAAE,OAAO,UAAU,KAAK,SAAS,CAAC;AAAA,UAC7D;AAAA;AAAA,UAEA,SAAS;AACP,kBAAM,aAAoB,KAAK;AAC/B,kBAAM,IAAI,MAAM,mCAAmC,UAAU,EAAE;AAAA,UACjE;AAAA,QACF;AAAA,MACF,CAAC;AAGD,eAAS,IAAI,GAAG,IAAI,QAAQ,QAAQ,KAAK;AACvC,wBAAgB,WAAW,CAAC,IAAI,QAAQ,CAAC;AAAA,MAC3C;AAAA,IACF;AAAA,EACF;AAGA,QAAM,cAAc,KAAK,IAAI,iBAAiB,MAAM,UAAU,CAAC;AAC/D,QAAM,UAAU,MAAM,KAAK,EAAE,QAAQ,YAAY,GAAG,MAAMD,QAAO,CAAC;AAClE,QAAM,QAAQ,IAAI,OAAO;AAEzB,SAAO,OAAO,IAAI,CAAC,GAAG,OAAO;AAAA,IAC3B,GAAG;AAAA,IACH,WAAW,gBAAgB,CAAC;AAAA,EAC9B,EAAE;AACJ;;;AJ/IA,eAAsB,YACpB,MACA,OAA4B,OACX;AACjB,QAAM,EAAE,cAAc,OAAAE,OAAM,IAC1B,OAAO,SAAS,YACZ,EAAE,cAAc,MAAM,OAAO,OAAU,IACvC,EAAE,cAAc,KAAK,gBAAgB,OAAO,OAAO,KAAK,MAAM;AAGpE,aAAW,OAAO,MAAM;AACtB,QAAI,CAAC,IAAI,QAAQ;AACf,UAAI,SAAS,MAAM,UAAU,IAAI,IAAI;AAAA,IACvC;AACA,QAAIA,QAAO;AACT,UAAI,SAAS,MAAM,YAAY,IAAI,QAAQA,MAAK;AAAA,IAClD;AAAA,EACF;AAGA,QAAM,eAAe,KAAK,IAAI,OAAK,eAAe,EAAE,IAAI,CAAC;AACzD,QAAM,SAAS,aAAa,OAAO,CAAC,KAAK,MAAM,MAAM,GAAG,CAAC;AAGzD,QAAM,cAAU,gBAAAC,SAAS,OAAO,EAAE,MAAM,EAAE,OAAO,EAAE,EAAE,CAAC;AACtD,QAAM,MAAM,IAAI,0BAAY;AAC5B,UAAQ,KAAK,GAAG;AAChB,QAAM,UAAoB,CAAC;AAC3B,MAAI,GAAG,QAAQ,WAAS,QAAQ,KAAK,KAAK,CAAC;AAE3C,QAAM,SAAS,OAAO;AAAA,IACpB,KAAK;AAAA,MACH;AAAA,QACE,SAAS;AAAA,QACT,UAAS,oBAAI,KAAK,GAAE,YAAY;AAAA,QAChC;AAAA,QACA;AAAA,QACA,WAAW;AAAA,MACb;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAAA,EACF;AAGA,UAAQ,OAAO,QAAQ,EAAE,MAAM,mBAAmB,OAAO,KAAK,CAAC;AAE/D,MAAI,cAAc;AAChB,eAAW,OAAO,MAAM;AACtB,YAAM,IAAK,IAAY;AACvB,UAAI,GAAG;AACL,gBAAQ,KAAK,GAAG,EAAE,MAAM,aAAa,IAAI,QAAQ,GAAG,CAAC;AAAA,MACvD,WAAY,IAAY,oBAAoB,QAAQ;AAClD,gBAAQ,OAAQ,IAAY,UAAU,EAAE,MAAM,aAAa,IAAI,QAAQ,GAAG,CAAC;AAAA,MAC7E;AAAA,IACF;AAAA,EACF;AAEA,QAAM,QAAQ,SAAS;AAEvB,SAAO,OAAO,OAAO,OAAO;AAC9B;;;AKtEA,sBAA+B;AAC/B,gBAA2B;AAU3B,IAAI;AAGJ,IAAM,iBAAN,MAAqB;AAAA,EAInB,YAAY,YAAoB,YAAY;AAH5C,SAAQ,QAA8D,CAAC;AAAA,EAG1B;AAAA;AAAA,EAI7C,KAAK,MAAoB;AAAA,EAAC;AAAA;AAAA,EAG1B,QAAQ,KAAa;AAGnB,UAAM,QAAQ,IAAI,KAAK,EAAE,YAAY;AAGrC,QAAI,MAAM,WAAW,QAAQ,GAAG;AAC9B,aAAO;AAAA,QACL,KAAK,CAAC,IAAY,QAAgB,aAAqB;AACrD,gBAAM,MAAM,KAAK,MAAM,UAAU,OAAK,EAAE,OAAO,EAAE;AACjD,gBAAM,MAAM,EAAE,IAAI,QAAQ,QAAQ,MAAM,SAAS;AACjD,cAAI,OAAO,EAAG,MAAK,MAAM,GAAG,IAAI;AAAA,cAAU,MAAK,MAAM,KAAK,GAAG;AAAA,QAC/D;AAAA,MACF;AAAA,IACF;AAKA,QAAI,MAAM,WAAW,QAAQ,GAAG;AAC9B,aAAO;AAAA,QACL,KAAK,MAAM,KAAK,MAAM,MAAM;AAAA,MAC9B;AAAA,IACF;AAGA,WAAO;AAAA,MACL,KAAK,MAAM;AAAA,MAAC;AAAA,MACZ,KAAK,MAAM,CAAC;AAAA,IACd;AAAA,EACF;AACF;AAEA,IAAI;AAEF,iBAAe,QAAQ,gBAAgB;AACzC,QAAQ;AACN,iBAAe;AACjB;AAwBO,IAAM,gBAAN,MAAoB;AAAA,EAKzB,YAAY,MAAkB;AAC5B,QAAI,CAAC,QAAQ,OAAO,KAAK,QAAQ,YAAY,KAAK,OAAO,GAAG;AAC1D,YAAM,IAAI,MAAM,iDAAiD;AAAA,IACnE;AAMA,QAAI;AACF,WAAK,KAAK,IAAI,aAAa,KAAK,UAAU,UAAU;AAAA,IACtD,QAAQ;AAEN,WAAK,KAAK,IAAI,eAAe;AAE7B,WAAK,aAAa;AAAA,IACpB;AACA,SAAK,MAAM,KAAK;AAGhB,QAAI,WAAW;AACf,QAAI;AAKF,MAAU,eAAK,KAAK,EAAE;AAAA,IACxB,QAAQ;AACN,iBAAW;AAAA,IACb;AACA,SAAK,aAAa;AAGlB,UAAM,UAAU;AAChB,SAAK,GAAG,KAAK,0CAA0C,OAAO,GAAG;AAKjE,QAAI;AACF,WAAK,GAAG,KAAK,wEAAwE;AAAA,IACvF,QAAQ;AAAA,IAER;AAAA,EACF;AAAA;AAAA,EAGA,OAAO,IAAY,QAAkB,OAAY,CAAC,GAAS;AACzD,QAAI,OAAO,WAAW,KAAK,KAAK;AAC9B,YAAM,IAAI,MAAM,4CAA4C,KAAK,GAAG,SAAS,OAAO,MAAM,GAAG;AAAA,IAC/F;AACA,UAAM,UAAc,uBAAM,MAAM;AAChC,UAAM,OAAO,KAAK,UAAU,QAAQ,CAAC,CAAC;AACtC,UAAM,OAAO,KAAK,GAAG;AAAA,MACnB;AAAA;AAAA,IAEF;AACA,SAAK,IAAI,IAAI,KAAK,IAAI;AAAA,EACxB;AAAA;AAAA,EAGA,OAAO,OAAiB,IAAI,GAAoD;AAC9E,QAAI,MAAM,WAAW,KAAK,KAAK;AAC7B,YAAM,IAAI,MAAM,2CAA2C,KAAK,GAAG,SAAS,MAAM,MAAM,GAAG;AAAA,IAC7F;AAKA,QAAI,CAAC,KAAK,YAAY;AACpB,YAAM,UAAM,uBAAM,KAAK;AACvB,YAAM,OAAc,KAAK,GACtB;AAAA,QACC;AAAA;AAAA;AAAA;AAAA,MAIF,EACC,IAAI,KAAK,CAAC;AAOb,YAAM,gBAAgB,KAAK,SAAS,KAAK,OAAO,KAAK,CAAC,EAAE,SAAS,YAAY,CAAC,OAAO,MAAM,KAAK,CAAC,EAAE,IAAI;AAEvG,UAAI,eAAe;AACjB,eAAO,KAAK,IAAI,QAAM;AAAA,UACpB,IAAI,EAAE;AAAA,UACN,OAAO,IAAK,EAAE;AAAA;AAAA,UACd,MAAM,EAAE,OAAO,KAAK,MAAM,EAAE,IAAc,IAAI;AAAA,QAChD,EAAE;AAAA,MACJ;AAAA,IAGF;AAGA,UAAM,MAAa,KAAK,GAAG,QAAQ,yCAAyC,EAAE,IAAI;AAClF,UAAM,UAAU,IAAI,IAAI,SAAO;AAC7B,YAAM,UAAgB,yBAAQ,IAAI,MAAgB;AAClD,YAAM,MAAM,KAAK,iBAAiB,OAAO,GAAG;AAC5C,aAAO,EAAE,IAAI,IAAI,IAAc,OAAO,KAAK,MAAM,IAAI,OAAO,KAAK,MAAM,IAAI,IAAc,IAAI,OAAU;AAAA,IACzG,CAAC;AAED,YAAQ,KAAK,CAAC,GAAG,MAAM,EAAE,QAAQ,EAAE,KAAK;AACxC,WAAO,QAAQ,MAAM,GAAG,CAAC;AAAA,EAC3B;AAAA;AAAA,EAGQ,iBAAiB,GAAa,GAAqB;AACzD,QAAI,MAAM;AACV,QAAI,OAAO;AACX,QAAI,OAAO;AACX,aAAS,IAAI,GAAG,IAAI,EAAE,QAAQ,KAAK;AACjC,aAAO,EAAE,CAAC,IAAI,EAAE,CAAC;AACjB,cAAQ,EAAE,CAAC,IAAI,EAAE,CAAC;AAClB,cAAQ,EAAE,CAAC,IAAI,EAAE,CAAC;AAAA,IACpB;AACA,QAAI,SAAS,KAAK,SAAS,EAAG,QAAO;AACrC,WAAO,OAAO,KAAK,KAAK,IAAI,IAAI,KAAK,KAAK,IAAI;AAAA,EAChD;AACF;;;AhB5MA,IAAI,QAAQ,aAAAC;AAEZ,eAAe,OAAO;AACpB,QAAM,UAAU,IAAI,yBAAQ;AAE5B,UACG,KAAK,YAAY,EACjB,YAAY,4CAA4C,EACxD,SAAS,eAAe,0CAAqC,EAC7D,OAAO,SAAS,oCAAoC,EACpD,OAAO,sBAAsB,mDAAmD,EAChF,OAAO,sBAAsB,8CAA8C,EAC3E,OAAO,SAAS,kDAAkD,EAClE,OAAO,kBAAkB,+BAA+B,EACxD,OAAO,0BAA0B,0CAA0C,EAC3E,OAAO,gBAAgB,kEAAkE,EACzF,OAAO,UAAU,8CAA8C,EAC/D,OAAO,cAAc,wBAAwB,EAC7C,OAAO,wBAAwB,gCAAgC,EAC/D,OAAO,eAAe,wCAAwC,EAC9D,YAAY,SAAS;AAAA;AAAA;AAAA;AAAA,CAIzB,EACI,MAAM;AAET,QAAM,OAAO,QAAQ,KAAK;AAC1B,QAAM,WAAW,QAAQ;AAGzB,MAAI;AAEJ,MAAI,CAAC,KAAK,QAAQ,SAAS,WAAW,GAAG;AACvC,YAAQ,MAAM,MAAM,IAAI,2BAA2B,CAAC;AACpD,YAAQ,KAAK,CAAC;AAAA,EAChB;AAGA,MAAI,KAAK,SAAS;AAChB,YAAQ,IAAI,mBAAM,EAAE,OAAO,EAAE,CAAC;AAAA,EAChC;AAGA,MAAI,KAAK,MAAM;AACb,UAAM,aAAa,YAAAC,QAAK,QAAQ,QAAQ,IAAI,GAAG,sBAAsB;AACrE,QAAI,SAAS;AACb,QAAI;AACF,YAAM,UAAAC,SAAG,OAAO,UAAU;AAC1B,eAAS;AAAA,IACX,QAAQ;AAAA,IAER;AAEA,QAAI,CAAC,QAAQ;AACX,YAAM,WAAW;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAkBjB,YAAM,UAAAA,SAAG,UAAU,YAAY,UAAU,MAAM;AAC/C,cAAQ,IAAI,MAAM,MAAM,WAAW,YAAAD,QAAK,SAAS,QAAQ,IAAI,GAAG,UAAU,CAAC,EAAE,CAAC;AAAA,IAChF,OAAO;AACL,cAAQ,IAAI,MAAM,OAAO,2DAAsD,CAAC;AAAA,IAClF;AAEA,YAAQ,KAAK,CAAC;AAAA,EAChB;AAGA,QAAM,UAAU,UAAM,iBAAAE,SAAG,UAAU,EAAE,UAAU,MAAM,KAAK,MAAM,CAAC;AACjE,QAAM,cAAc,MAAM,KAAK,IAAI,IAAI,OAAO,CAAC;AAE/C,MAAI,YAAY,WAAW,GAAG;AAC5B,YAAQ,MAAM,MAAM,IAAI,4CAA4C,CAAC;AACrE,YAAQ,KAAK,CAAC;AAAA,EAChB;AAGA,QAAM,WAAW,IAAI,oBAAAC,QAAY;AAAA,IAC/B;AAAA,MACE,iBAAiB;AAAA,MACjB,YAAY;AAAA,MACZ,QAAQ;AAAA,IACV;AAAA,IACA,oBAAAA,QAAY,QAAQ;AAAA,EACtB;AAEA,QAAM,WAAW,SAAS,OAAO,YAAY,QAAQ,GAAG,EAAE,OAAO,QAAQ,CAAC;AAC1E,QAAM,WAAW,KAAK,QAAQ,SAAS,OAAO,YAAY,QAAQ,GAAG,EAAE,OAAO,QAAQ,CAAC,IAAI;AAY3F,QAAM,UAAqB,CAAC;AAE5B,MAAI,QAAQ;AAEZ,aAAW,QAAQ,aAAa;AAC9B,QAAI;AACF,YAAM,MAAM,MAAM,UAAAF,SAAG,SAAS,IAAI;AAClC,YAAM,YAA+C;AAAA,QACnD,cAAc,KAAK;AAAA,QACnB,OAAO,KAAK,QAEN,KAAK,UAAU,WACX;AAAA,UACE,UAAU;AAAA,UACV,QAAQ,KAAK;AAAA,UACb,OAAO,KAAK;AAAA,QACd,IACA;AAAA,UACE,UAAU;AAAA,UACV,UAAU,KAAK;AAAA,UACf,OAAO,KAAK;AAAA,QACd,IAEN;AAAA,QACJ,KAAK,KAAK;AAAA,MACZ;AAEA,YAAM,MAAM,MAAM,UAAU,KAAK,SAAS;AAC1C,UAAI,WAAW,YAAAD,QAAK,SAAS,IAAI;AAGjC,UAAI,CAAC,IAAI,QAAQ;AACf,gBAAQ,MAAM,MAAM,OAAO,cAAc,IAAI,mCAAmC,CAAC;AACjF,gBAAQ,KAAK,EAAE,MAAM,QAAQ,GAAG,QAAQ,GAAG,YAAY,GAAG,OAAO,GAAG,SAAS,KAAK,CAAC;AACnF;AAAA,MACF;AAEA,YAAM,SAAS,eAAe,IAAI,IAAI;AACtC,YAAM,SAAS,IAAI,OAAO;AAC1B,YAAM,aAAa,IAAI,OAAO,OAAO,OAAM,EAAU,UAAU,MAAS,EAAE;AAG1E,YAAM,SAAS,MAAM,YAAY,CAAC,GAAG,GAAG,SAAS;AAGjD,UAAI,KAAK,aAAa,WAAW,KAAK,SAAS,IAAI,QAAQ;AACzD,mBAAW,SAAS,IAAI,QAAQ;AAC9B,cAAI,CAAC,MAAM,UAAW;AAGtB,cAAI,CAAC,KAAK;AACR,kBAAM,IAAI,cAAc,EAAE,KAAK,MAAM,UAAU,QAAQ,QAAQ,KAAK,GAAG,CAAC;AAAA,UAC1E;AAEA,gBAAM,UAAU,GAAG,IAAI,MAAM,YAAAA,QAAK,SAAS,IAAI,CAAC,IAAI,MAAM,GAAG;AAC7D,cAAI,OAAO,SAAS,MAAM,WAAW,EAAE,MAAM,IAAI,UAAU,UAAU,MAAM,IAAI,CAAC;AAAA,QAClF;AAAA,MACF;AAGA,UAAI,UAAU;AACZ,iBAAS,UAAU,GAAG,EAAE,OAAO,QAAQ,CAAC;AAAA,MAC1C;AAGA,UAAI;AACJ,UAAI,KAAK,OAAO,YAAY,WAAW,GAAG;AACxC,eAAO,YAAAA,QAAK,QAAQ,OAAO,KAAK,GAAG,CAAC;AAAA,MACtC,OAAO;AACL,eAAO,YAAAA,QAAK;AAAA,UACV,YAAAA,QAAK,QAAQ,IAAI;AAAA,UACjB,GAAG,YAAAA,QAAK,SAAS,MAAM,YAAAA,QAAK,QAAQ,IAAI,CAAC,CAAC;AAAA,QAC5C;AAAA,MACF;AACA,YAAM,UAAAC,SAAG,UAAU,MAAM,MAAM;AAC/B,YAAM,QAAQ,OAAO;AAErB,cAAQ,KAAK,EAAE,MAAM,QAAQ,QAAQ,YAAY,OAAO,SAAS,MAAM,CAAC;AAAA,IAC1E,SAAS,KAAK;AACZ,cAAQ,KAAK,EAAE,MAAM,QAAQ,GAAG,QAAQ,GAAG,YAAY,GAAG,OAAO,GAAG,SAAS,MAAM,OAAO,IAAI,CAAC;AAC/F,cAAQ,MAAM,MAAM,IAAI,oBAAoB,IAAI,KAAK,eAAe,QAAQ,IAAI,UAAU,GAAG,EAAE,CAAC;AAAA,IAElG,UAAE;AAEA,eAAS,UAAU,GAAG,EAAE,OAAO,QAAQ,CAAC;AAAA,IAC1C;AAAA,EACF;AAEA,WAAS,KAAK;AAGd,QAAM,iBAAiB,QAAQ,OAAO,OAAK,CAAC,EAAE,OAAO,EAAE;AACvD,QAAM,eAAe,QAAQ,OAAO,OAAK,EAAE,OAAO,EAAE;AAEpD,UAAQ,IAAI,MAAM,KAAK,WAAW,CAAC;AACnC,UAAQ;AAAA,IACN,QAAQ,IAAI,QAAM;AAAA,MAChB,MAAM,YAAAD,QAAK,SAAS,EAAE,IAAI;AAAA,MAC1B,QAAQ,EAAE;AAAA,MACV,QAAQ,EAAE;AAAA,MACV,YAAY,EAAE;AAAA,MACd,OAAO,EAAE;AAAA,MACT,SAAS,EAAE,UAAU,QAAQ;AAAA,IAC/B,EAAE;AAAA,EACJ;AAEA,MAAI,eAAe,GAAG;AACpB,YAAQ,MAAM,MAAM,OAAO,GAAG,YAAY,yCAAyC,CAAC;AACpF,YAAQ,KAAK,CAAC;AAAA,EAChB,WAAW,OAAO;AAChB,YAAQ,KAAK,CAAC;AAAA,EAChB,OAAO;AACL,YAAQ,KAAK,CAAC;AAAA,EAChB;AACF;AAEA,KAAK,EAAE,MAAM,SAAO;AAClB,UAAQ,MAAM,MAAM,IAAI,eAAe,QAAQ,IAAI,SAAS,IAAI,UAAU,OAAO,GAAG,CAAC,CAAC;AACtF,UAAQ,KAAK,CAAC;AAChB,CAAC;","names":["mime","mime","fs","uuid","crypto","import_uuid","import_node_fs","fs","unzipper","mammoth","uuid","sax","import_uuid","import_node_fs","fs","chardet","iconv","uuid","import_uuid","uuid","mime","import_node_fs","path","parser","doc","fs","embed","worker","embed","embed","archiver","chalkPkg","path","fs","fg","cliProgress"]}